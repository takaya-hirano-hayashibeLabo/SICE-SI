\makeatletter % @が使える
\subsection{実験2 : 入力の時間変動に対するモデル性能評価}

各モデルの学習後のテスト精度と1epochあたりの学習時間を\Table{sec4:tab:exp2}に示す.
提案手法のモデルは他のモデルと比較して, 短い学習時間であった.
これは提案手法は基準のタイムスケールのみを学習するためである.
提案手法は入力に対して, 時定数と膜電位を推定する回帰モデルを生成する必要があるが, その計算時間は数秒程度であり, ニューラルネットワークの学習時間に対して非常に短いため無視できる.
提案手法は入力の特徴抽出と推論をSNNで行い, 入力の時間変動に対する調整を計算コストの軽い回帰モデルで行うことで, 学習効率の良い特性を示した.
\begin{table}[htb]
    \centering
    \caption{各モデルの学習結果と学習時間}
    \label{sec4:tab:exp2}
    \begin{tabular}{cccc}
        \hline
        \textbf{model}&\textbf{test accuracy}&\textbf{seconds/epoch}\\
        \hline
        LSTM&$0.92\pm0.07$&$120.1\pm5.8$\\
        SNN&$0.86\pm0.06$&$209.4\pm7.2$\\
        ParametricSNN&$0.88\pm0.08$&$224.4\pm7.8$\\
        Proposed method&$0.92\pm0.09$&$94.8\pm3.0$\\
    \end{tabular}
\end{table}

入力の時間変動と各モデルのモデル精度変化を\Fig{sec4:fig:exp2:1}に示す.
LSTMとSNNは, 入力のタイムスケールの増加に従って, モデル精度が大きく低下していることがわかる.
ParametricSNNは, $a=6.0$まではモデル精度の劣化を抑えられているが, それより大きいタイムスケールでは精度が低下した.
一方, 提案手法は$a=10.0$までは精度を維持し, それより大きい場合であっても0.85以上のモデル精度を保つ結果となった.

入力シーケンス内の時間変動とモデル精度の関係を\Fig{sec4:fig:exp2:2}に示す.
学習に使用した$a=1.0$の入力が含まれるcaseA, caseBにおいては, caseBのSNNを除く全てのモデルが0.8以上のモデル精度であった.
一方で, 学習を行っていない入力で構成されるcaseC, caseDにおいては, 提案手法のみが0.8以上のモデル精度を維持する結果となった.
これらの結果から, 提案手法は学習コストを抑えつつ, 入力の時間変動に対して頑健な特性を持つ結果となった.